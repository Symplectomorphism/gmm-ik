\section{Solution of the Problem}
\label{sec:solution}

The solution will be to utilize a Gaussian Mixture Model (GMM) to represent the
inverse kinematics map. My presentation will closely follow three
sources:~\cite{mclachlan2007algorithm,ghahramani1993solving,xu2017data}.

We assume that the data $\Xi = \left\{ \xi_1, \ldots, \xi_N \right\}$ were
generated independenty by a mixture density:
%
\begin{equation}
    P(\xi_i) = \sum_{j=1}^M \underbrace{P(\omega_j)}_{\pi_j} 
        P(\xi_i \mid \omega_j; \psi_j),
    \label{eq:mixture_density}
\end{equation}
%
where $\{\pi_j\}_j^M$ are the mixture proportions, each component of the mixture
is denoted $\omega_j$ and parametrized by $\psi_j$. The mixture proportions are
to be picked such that \[ \sum_{j=1}^M \pi_j = 1. \] The full set of parameters
of this model are $\pi_j$'s and $\psi_j$'s. We will write $\Psi = (\pi_j,
\psi_j)_j$. When we restrict these probability distributions to be Gaussian, we
will further have $\psi_j = (\mu_j, \Sigma_j)$, where $\mu_j$ is the mean and
$\Sigma_j$ the covariance of the $j^{\textrm{th}}$ Gaussian. The log of the
likelihood of the parameters given the data set is
%
\begin{equation}
    \ell(\Psi \mid \Xi) = \log{ \Pi_{i=1}^N \sum_{j=1}^M P(\xi_i \mid \omega_j; 
    \psi_j) P(\omega_j) } = \sum_{i=1}^N \log{ \sum_{j=1}^M 
    P(\xi_i \mid \omega_j; \psi_j) P(\omega_j). }
    \label{eq:loglike}
\end{equation}
%
We seek to find the parameter vector $\Psi$ that maximizes $\ell(\Psi \mid
\Xi)$. However, this function is not easily maximized numerically because it
involves the log of a sum. Intuitively, it is not easily maximized because for
each data point, there is a ``credit-assignment'' problem, i.e., it is not clear
which component of the mixture generated the data point and thus which
parameters to adjust to fit that data point.

The expectation maximization (EM) algorithm applied to mixtures is an iterative
method for overcoming this credit-assignment problem. The intuition behind it is
that if one had access to a ``hidden'' random variable $z$ that indicated which
data point was generated by which component, then the maximization problem would
decouple into a set of simple maximizations. Mathematically, given $\mc{Z} =
\{z_1, \ldots, z_N \}$ a ``complete-data'' log likelihood function could be
written,
%
\begin{equation}
    \ell_c(\Psi \mid \Xi, \mc{Z}) = \sum_{i=1}^N \sum_{j=1}^M z_{ij} \log{
        P(\xi_i \mid z_i; \Psi)P(z_i; \Psi),
    }
    \label{eq:complete_loglike}
\end{equation}
%
such that it does not involve a log of a summation.

As proven in~\cite{dempster1977maximum}, $\ell(\Psi \mid \Xi)$ can be maximized
by iterating the following two steps,
%
\begin{align}
    \begin{split}
    \text{E-step: } \quad &Q(\Psi \mid \Psi_k) = \mathbb{E}\left[ \ell_c(\Psi \mid \Xi, \mc{Z}) \mid \Xi, \Psi_k \right] \\
    \text{M-step: } \quad &\Psi_{k+1} = \arg \max_\Psi Q(\Psi \mid \Psi_k).
    \end{split}
    \label{eq:EM_alg}
\end{align}
%
The E (Expectation) step computes the expected complete data log likelihood and 
the M (Maximization) step finds the parameters that maximize this likelihood.

\subsection{Mixture of Gaussians}
Let me now specialize to the case where the probability functions above are
Gaussians. For this model, the E-step simplifies to computing $h_{ij} =
\mathbb{E}\left[ z_{ij} \mid \xi_i, \psi_k \right]$, the probability that
Gaussian $j$, as defined by the mean $\hat{\mu}_j$ and covariance matrix
$\hat{\Sigma}_j$ estimated at time step $k$, generated data point $i$:
%
\begin{equation}
    h_{ij}^k = \frac{\abs{\hat{\Sigma}_j^k}^{-\frac{1}{2}} 
    \exp{\left\{-\frac{1}{2}\left( \xi_i - \hat{\mu}_j^k \right)^\top 
    ({\hat{\Sigma}_j^k})^{-1} \left( \xi_i - \hat{\mu}_j^k \right) \right\}}  }
    {\sum_{l=1}^M \abs{\hat{\Sigma}_j^k}^{-\frac{1}{2}} 
    \exp{\left\{-\frac{1}{2}\left( \xi_i - \hat{\mu}_l^k \right)^\top 
    ({\hat{\Sigma}_l^k})^{-1} \left( \xi_i - \hat{\mu}_l^k \right) \right\}} }.
    \label{eq:E-step}
\end{equation}
%
The M-step then involves re-estimating the means and covariances of the
Gaussians along with the mixing proportions using the data set weighted by the
$h_{ij}$:
%
\begin{equation}
    \hat{\mu}_j^{k+1} = \frac{\sum_{i=1}^N h_{ij}^k\xi_i}{\sum_{i=1}^N h_{ij}^k}, \quad
    \hat{\Sigma}_j^{k+1} = \frac{\sum_{i=1}^N h_{ij}^k \left(\xi_i - \hat{\mu}_j^{k+1}\right)
                    \left(\xi_i - \hat{\mu}_j^{k+1}\right)^\top}{\sum_{i=1}^N h_{ij}^k}, \quad
    \pi_j^{k+1} = \frac{1}{N}\sum_{i=1}^N h_{ij}^k.
    \label{eq:M-step}
\end{equation}


\subsection{Supervised Learning}
%
When viewed as supervised learning, each vector $\xi_i$ in the training set is
composed of an ``input'' subvector $\mathrm{x}_i$ and a ``target'' or output
subvector $\theta_i$. Applying the learning algorithm, we obtain an estimate of
the density of the data in this input/output space. This estimate can be used to
approximate a function in the following way:

Given the input vector $\mathrm{x}_i$, we extract all the relevant information
from the joint probability density function (pdf) $P(\mathrm{x}, \theta)$ by
marginalizing to $P(\theta \mid \mathrm{x})$. For a single Gaussian this
conditional density is normal, and by linearity, since $P(\mathrm{x}, \theta)$
is a mixture of Gaussians, so is $P(\theta \mid \mathrm{x})$. In principle, this
conditional density is the final output of the density estimator. That is, given
a particular input, the network returns the complete conditional density of the
output. However, for the purposes of comparison to function approximation
methods and since many applications require a single estimate of the output, I
will outline two possible ways to obtain such an estimate $\hat{\theta}_i$ of
$\theta_i = f(x_i)$.